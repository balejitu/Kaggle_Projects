{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\nimport tensorflow as tf\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom matplotlib import pyplot as plt\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "06b8f157e00cb0cb6c19f5d683e459d857eae8e0"
      },
      "cell_type": "code",
      "source": "pd.read_csv(\"../input/sampleSubmission.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "def cost(logits, labels):\n    z = tf.placeholder(tf.float32, name=\"z\")\n    y = tf.placeholder(tf.float32, name=\"y\")\n    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits=z, labels=y)\n    sess = tf.Session()\n    result = sess.run(cost)\n    sess.close()\n    return result",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d41b69953313df2006586ef5a440245115ddc343"
      },
      "cell_type": "code",
      "source": "def one_hot_matrix(labels, C):\n    C = tf.constant(C, name=\"C\")\n    one_hot_matrix = tf.one_hot(labels, C, axis=0)\n    sess = tf.Session()\n    one_hot = sess.run(one_hot_matrix)\n    \n    sess.close()    \n    return one_hot",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f7e616cab3579a4622912c81d10071ee83c48b3"
      },
      "cell_type": "code",
      "source": "def ones(shape):\n    ones=tf.ones(shape)\n    sess=tf.Session()\n    ones = sess.run(ones)\n    sess.close()\n    return ones\nimport os\nimport sys\nfrom IPython.display import display\nfrom IPython.display import Image as _Imgdis\nfrom scipy import ndimage\nfolder = \"../input/train/train\"\n\nonlyfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n\nprint(\"Working with {0} images\".format(len(onlyfiles)))\nprint(\"Image examples: \")\n\n\"\"\"\"for i in range(4, 5):\n    \n    display(_Imgdis(filename=folder + \"/\" + onlyfiles[i], width=240, height=320))\"\"\"\ndef random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    mini_batch_size - size of the mini-batches, integer\n    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(seed)\n    \n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((Y.shape[0],m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : k * mini_batch_size + mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches\n\ndef convert_to_one_hot(Y, C):\n    Y = np.eye(C)[Y.reshape(-1)].T\n    return Y\n\ndef predict(X, parameters):\n    \n    W1 = tf.convert_to_tensor(parameters[\"W1\"], dtytpe=tf.float32)\n    b1 = tf.convert_to_tensor(parameters[\"b1\"], dtytpe=tf.float32)\n    W2 = tf.convert_to_tensor(parameters[\"W2\"], dtytpe=tf.float32)\n    b2 = tf.convert_to_tensor(parameters[\"b2\"], dtytpe=tf.float32)\n    W3 = tf.convert_to_tensor(parameters[\"W3\"], dtytpe=tf.float32)\n    b3 = tf.convert_to_tensor(parameters[\"b3\"], dtytpe=tf.float32)\n    \n    params = {\"W1\": W1,\n              \"b1\": b1,\n              \"W2\": W2,\n              \"b2\": b2,\n              \"W3\": W3,\n              \"b3\": b3}\n    \n    x = tf.placeholder(\"float\", [9216, 1])\n    \n    z3 = forward_propagation(x, params)\n    p = tf.argmax(z3)\n    \n    with tf.Session() as sess:\n        prediction = sess.run(p, feed_dict = {x: X})\n        \n    return prediction\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ac004f7707d8b99729826f71ee1610060f93e612",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "from scipy import ndimage\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\ndata1 = []\ndata2 = []\nlabel = []\nlabel2 = []\ncounter=0\ni=0\nfor file in onlyfiles:\n    if counter>=12000:\n        image_data=cv2.imread(os.path.join(folder,file), cv2.IMREAD_GRAYSCALE)\n        image_data=cv2.resize(image_data,(96,96))\n        if file.startswith(\"cat\"):\n            label2.append(0)\n        elif file.startswith(\"dog\"):\n            label2.append(1)\n            \n        try:\n            data2.append(image_data/255)\n        except:\n            label=label[:len(label)-1]\n        counter+=1\n    else:\n        image_data=cv2.imread(os.path.join(folder,file), cv2.IMREAD_GRAYSCALE)\n        image_data=cv2.resize(image_data,(96,96))\n        if file.startswith(\"cat\"):\n            label.append(0)\n        elif file.startswith(\"dog\"):\n            label.append(1)\n        try:\n            data1.append(image_data/255)\n        except:\n            label=label[:len(label)-1]\n        counter+=1\n    \n        \nprint(\"Data Received\")\n        \n        ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e2b0d37431bf8405addde2e5ae9c575756fe5501"
      },
      "cell_type": "code",
      "source": "\nprint(\"Files in train_files: %d\" % len(data1))\nprint(\"Files in test_file: %d\" %len(data2))\nprint(np.shape(label))\nprint(np.shape(label2))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a06a33b21b7fba6e612e5e1b117659d90b9cc02e"
      },
      "cell_type": "code",
      "source": "import numpy as np\n\ntrain_data1=np.array(data1)\ntrain_data1.astype(np.float64)\ntrain_data1=train_data1.reshape((train_data1.shape)[0],(train_data1.shape)[1]*(train_data1.shape)[2])\ntrain_labels=np.array(label)\ntrain_labels.astype(np.float64)\ntest_data2 = np.array(data2)\ntest_data2.astype(np.float64)\ntest_data2 = test_data2.reshape((test_data2.shape)[0],(test_data2.shape)[1]*(test_data2.shape)[2])\ntest_labels = np.array(label2)\ntest_labels.astype(np.float64)\ntrain_data1 = np.concatenate((train_data1, test_data2),axis=0)\ntrain_labels = np.concatenate((train_labels, test_labels),axis=0)\nprint (train_data1.shape)\nprint (test_data2.shape)\nprint (train_labels.shape)\nprint (test_labels.shape)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bb686411a1343f771e6356a362746c8cc26064e8"
      },
      "cell_type": "code",
      "source": "test_data=[]\nid=[]\ncounter=0\nfor file in os.listdir(\"../input/test1/test1\"):\n    image_data=cv2.imread(os.path.join(\"../input/test1/test1\",file), cv2.IMREAD_GRAYSCALE)\n    try:\n        image_data=cv2.resize(image_data,(96,96))\n        test_data.append(image_data/255)\n        id.append((file.split(\".\"))[0])\n    except:\n        print (\"ek gaya\")\n    counter+=1\n    if counter%1000==0:\n        print (counter,\" image data retreived\")\n        \ntest_data1=np.array(test_data)\ntest_data1.astype(np.float64)\nprint (test_data1.shape)\ntest_data1=test_data1.reshape((test_data1.shape)[0],(test_data1.shape)[1]*(test_data1.shape)[2])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9561996a1427c001f2f89e2f37131e76b1d832d3"
      },
      "cell_type": "code",
      "source": "print(test_data1.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2e13c83af76c20719bed507fb03f1e3aaf163db"
      },
      "cell_type": "code",
      "source": "X_train_orig = train_data1\nY_train_orig = train_labels\nX_test_orig=test_data2\nY_test_orig = test_labels\nprint(\"Done\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4021ddcabfa0ed8d71a960ef5f4c55f96e0f525"
      },
      "cell_type": "code",
      "source": "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\nX_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n# Normalize image vectors\nX_train = X_train_flatten\nX_test = X_test_flatten\n# Convert training and test labels to one hot matrices\nY_train = one_hot_matrix(Y_train_orig,2 )\nY_test = one_hot_matrix(Y_test_orig, 2)\n\nprint (\"number of training examples = \" + str(X_train.shape[1]))\nprint (\"number of test examples = \" + str(X_test.shape[1]))\nprint (\"X_train shape: \" + str(X_train.shape))\nprint (\"Y_train shape: \" + str(Y_train.shape))\nprint (\"X_test shape: \" + str(X_test.shape))\nprint (\"Y_test shape: \" + str(Y_test.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e8a5298509fdbe03785c740afd508a2c7229c78c"
      },
      "cell_type": "code",
      "source": "def create_placeholders(n_x, n_y):\n    \"\"\"\n    Creates the placeholders for the tensorflow session.\n    \n    Arguments:\n    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n    n_y -- scalar, number of classes (from 0 to 5, so -> 6)\n    \n    Returns:\n    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n    \n    Tips:\n    - You will use None because it let's us be flexible on the number of examples you will for the placeholders.\n      In fact, the number of examples during test/train is different.\n    \"\"\"\n\n    ### START CODE HERE ### (approx. 2 lines)\n    X = tf.placeholder(tf.float32,shape=(n_x,None))\n    Y = tf.placeholder(tf.float32,shape=(n_y,None))\n    ### END CODE HERE ###\n    \n    return X, Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "97def0b2a78f2636cf77af6ecd14037cef80b8fe"
      },
      "cell_type": "code",
      "source": "def initialize_parameters():\n    \"\"\"\n    Initializes parameters to build a neural network with tensorflow. The shapes are:\n                        W1 : [25, 12288]\n                        b1 : [25, 1]\n                        W2 : [12, 25]\n                        b2 : [12, 1]\n                        W3 : [6, 12]\n                        b3 : [6, 1]\n    \n    Returns:\n    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n    \"\"\"\n    \n    tf.set_random_seed(1)                   # so that your \"random\" numbers match ours\n        \n    ### START CODE HERE ### (approx. 6 lines of code)\n    W11 = tf.get_variable(\"W1\", [25,9216], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b11 = tf.get_variable(\"b1\", [25,1], initializer = tf.zeros_initializer())\n    W22 = tf.get_variable(\"W2\", [12,25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b22 = tf.get_variable(\"b2\", [12,1], initializer = tf.zeros_initializer())\n    W33 = tf.get_variable(\"W3\", [2,12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n    b33 = tf.get_variable(\"b3\", [2,1], initializer = tf.zeros_initializer())\n    ### END CODE HERE ###\n\n    parameters = {\"W1\": W11,\n                  \"b1\": b11,\n                  \"W2\": W22,\n                  \"b2\": b22,\n                  \"W3\": W33,\n                  \"b3\": b33}\n    \n    return parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "15deec0fafdc9b71a2c1c28c753f39f4828806ea"
      },
      "cell_type": "code",
      "source": "def forward_propagation(X, parameters):\n    \"\"\"\n    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n                  the shapes are given in initialize_parameters\n\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    \"\"\"\n    \n    # Retrieve the parameters from the dictionary \"parameters\" \n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    \n    ### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:\n    Z1 = tf.add(tf.matmul(W1,X),b1)                                              # Z1 = np.dot(W1, X) + b1\n    A1 = tf.nn.relu(Z1)                                              # A1 = relu(Z1)\n    Z2 = tf.add(tf.matmul(W2,A1),b2)                                              # Z2 = np.dot(W2, a1) + b2\n    A2 = tf.nn.relu(Z2)                                              # A2 = relu(Z2)\n    Z3 = tf.add(tf.matmul(W3,A2),b3)                                              # Z3 = np.dot(W3,Z2) + b3\n    ### END CODE HERE ###\n    \n    return Z3",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1773ffc87ff598be533f474e52ba9e5161130c0"
      },
      "cell_type": "code",
      "source": "def compute_cost(Z3, Y):\n    \"\"\"\n    Computes the cost\n    \n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    \n    Returns:\n    cost - Tensor of the cost function\n    \"\"\"\n    \n    # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    \n    ### START CODE HERE ### (1 line of code)\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits))\n    ### END CODE HERE ###\n    \n    return cost",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aaacac4c5d0669d0852b930cf523cbce4279cd7f"
      },
      "cell_type": "code",
      "source": "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n          num_epochs = 1500, minibatch_size = 32, print_cost = True):\n    \"\"\"\n    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n    \n    Arguments:\n    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every 100 epochs\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    tf.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n    tf.set_random_seed(1)                             # to keep consistent results\n    seed = 3                                          # to keep consistent results\n    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n    n_y = Y_train.shape[0]                            # n_y : output size\n    costs = []                                        # To keep track of the cost\n    # Create Placeholders of shape (n_x, n_y)\n    ### START CODE HERE ### (1 line)\n    X, Y = create_placeholders(n_x, n_y)\n    ### END CODE HERE ###\n\n    # Initialize parameters\n    ### START CODE HERE ### (1 line)\n    parameters = initialize_parameters()\n    ### END CODE HERE ###\n    \n    # Forward propagation: Build the forward propagation in the tensorflow graph\n    ### START CODE HERE ### (1 line)\n    Z3 = forward_propagation(X, parameters)\n    ### END CODE HERE ###\n    \n    # Cost function: Add cost function to tensorflow graph\n    ### START CODE HERE ### (1 line)\n    cost = compute_cost(Z3,Y)\n    ### END CODE HERE ###\n    \n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n    ### START CODE HERE ### (1 line)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(cost)\n    ### END CODE HERE ###\n    \n    # Initialize all the variables\n    init = tf.global_variables_initializer()\n\n    # Start the session to compute the tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run the initialization\n        sess.run(init)\n        \n        # Do the training loop\n        for epoch in range(num_epochs):\n\n            epoch_cost = 0.                       # Defines a cost related to an epoch\n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\n            seed = seed + 1\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n\n            for minibatch in minibatches:\n\n                # Select a minibatch\n                (minibatch_X, minibatch_Y) = minibatch\n                \n                # IMPORTANT: The line that runs the graph on a minibatch.\n                # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n                ### START CODE HERE ### (1 line)\n                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X, Y: minibatch_Y})\n                ### END CODE HERE ###\n                \n                epoch_cost += minibatch_cost / num_minibatches\n\n            # Print the cost every epoch\n            if print_cost == True and epoch % 100 == 0:\n                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n            if print_cost == True and epoch % 5 == 0:\n                costs.append(epoch_cost)\n                \n        # plot the cost\n        plt.plot(np.squeeze(costs))\n        plt.ylabel('cost')\n        plt.xlabel('iterations (per tens)')\n        plt.title(\"Learning rate =\" + str(learning_rate))\n        plt.show()\n\n        # lets save the parameters in a variable\n        parameters = sess.run(parameters)\n        print (\"Parameters have been trained!\")\n\n        # Calculate the correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n\n        # Calculate accuracy on the test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n\n        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n        print (\"Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n        \n        return parameters",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11c3e33847c56121977da61fced48f2b3ef6f33f",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "import math\nparameters = model(X_train, Y_train, X_test, Y_test)\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}